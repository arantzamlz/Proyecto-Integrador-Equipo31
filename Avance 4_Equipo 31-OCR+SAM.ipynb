{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m978.2/978.2 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.6/300.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "--2026-02-23 23:32:22--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.167.112.129, 3.167.112.66, 3.167.112.53, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.167.112.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 375042383 (358M) [binary/octet-stream]\n",
      "Saving to: ‘sam_vit_b.pth’\n",
      "\n",
      "sam_vit_b.pth       100%[===================>] 357.67M   272MB/s    in 1.3s    \n",
      "\n",
      "2026-02-23 23:32:24 (272 MB/s) - ‘sam_vit_b.pth’ saved [375042383/375042383]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  INSTALACIÓN DE DEPENDENCIAS\n",
    "\n",
    "!pip install segment-anything opencv-python pytesseract transformers easyocr tqdm pandas --quiet\n",
    "!pip install git+https://github.com/facebookresearch/segment-anything.git --quiet\n",
    "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth -O sam_vit_b.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "ZIP extraído en: /content/drive/MyDrive/Colab Notebooks/MNA/Proyecto Integrador/dataset_bimbonet\n",
      "Carpeta de imágenes: /content/drive/MyDrive/Colab Notebooks/MNA/Proyecto Integrador/dataset_bimbonet/20260121 imagenes Cashcollection\n",
      "Ejemplo de archivos: ['387.png', '388.png', '39.png', '391.png', '390.png']\n"
     ]
    }
   ],
   "source": [
    "#MONTAR GOOGLE DRIVE Y EXTRAER ZIP\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = \"/content/drive/MyDrive/Colab Notebooks/MNA/Proyecto Integrador/Copia de 20260121 imagenes Cashcollection1.zip\"\n",
    "extract_dir = \"/content/drive/MyDrive/Colab Notebooks/MNA/Proyecto Integrador/dataset_bimbonet\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    z.extractall(extract_dir)\n",
    "\n",
    "print(\"ZIP extraído en:\", extract_dir)\n",
    "\n",
    "IMAGE_DIR = os.path.join(extract_dir, \"20260121 imagenes Cashcollection\")\n",
    "print(\"Carpeta de imágenes:\", IMAGE_DIR)\n",
    "\n",
    "print(\"Ejemplo de archivos:\", os.listdir(IMAGE_DIR)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "#IMPORTACIÓN DE LIBRERÍAS Y CONFIGURACIÓN\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "import easyocr\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f919787ae62442e489c1f19da3a110bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/457 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "The image processor of type `GroundingDinoImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3528ae009f014f41b58b24bbeabe9c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903f5e86f1924b359409333c93b0d2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880ff075338c4f9b8a7766d0f81468de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7643f980e89146c3ac7e4b3c884bf3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd0d30fadc241f6acc1e2e6e94164e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ec2b5348bd44f689c2f37e62c21b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/933M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482f2d6c6eed41d59981658dced77ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1206 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
     ]
    }
   ],
   "source": [
    "#CARGA DE MODELOS (SAM, DINO, EASYOCR)\n",
    "\n",
    "# SAM\n",
    "sam = sam_model_registry[\"vit_b\"](checkpoint=\"sam_vit_b.pth\").to(device)\n",
    "mask_generator = SamAutomaticMaskGenerator(\n",
    "    model=sam,\n",
    "    points_per_side=4,\n",
    "    pred_iou_thresh=0.88,\n",
    "    stability_score_thresh=0.90,\n",
    "    min_mask_region_area=300\n",
    ")\n",
    "\n",
    "# DINO\n",
    "processor = AutoProcessor.from_pretrained(\"IDEA-Research/grounding-dino-base\")\n",
    "dino = AutoModelForZeroShotObjectDetection.from_pretrained(\n",
    "    \"IDEA-Research/grounding-dino-base\"\n",
    ").to(device)\n",
    "\n",
    "# EasyOCR\n",
    "reader = easyocr.Reader(['es', 'en'], gpu=(device==\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCIÓN DE REGEX PARA EXTRAER TEXTO\n",
    "\n",
    "def extract_col_strict(text):\n",
    "    text = text.upper()\n",
    "    pattern = r\"C[O0]L\\s*([123])[\\.\\,\\s]*(\\d{5})\\s+(\\d{5})\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    return [f\"COL {m[0]}. {m[1]} {m[2]}\" for m in matches]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCIÓN DE EXTRACCIÓN CON SAM\n",
    "\n",
    "def sam_extract(img):\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_small = cv2.resize(img_rgb, (1024, 1024))\n",
    "\n",
    "    masks = mask_generator.generate(img_small)\n",
    "    found = []\n",
    "\n",
    "    for m in masks:\n",
    "        mask = m[\"segmentation\"].astype(np.uint8) * 255\n",
    "        x, y, w, h = cv2.boundingRect(mask)\n",
    "        crop = img_small[y:y+h, x:x+w]\n",
    "\n",
    "        if crop.size == 0:\n",
    "            continue\n",
    "\n",
    "        ocr = pytesseract.image_to_string(crop)\n",
    "        cleaned = extract_col_strict(ocr)\n",
    "        if cleaned:\n",
    "            found.extend(cleaned)\n",
    "\n",
    "    return sorted(list(set(found)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCIÓN DE FALLBACK CON DINO\n",
    "\n",
    "def dino_fallback(img):\n",
    "    if img is None or img.size == 0:\n",
    "        return []\n",
    "\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    inputs = processor(images=img_rgb, text=\"printed text with numbers\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = dino(**inputs)\n",
    "\n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs, inputs.input_ids,\n",
    "        threshold=0.22, text_threshold=0.22,\n",
    "        target_sizes=[img.shape[:2]]\n",
    "    )[0]\n",
    "\n",
    "    found = []\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    for box in results[\"boxes\"]:\n",
    "        x1, y1, x2, y2 = map(int, box.tolist())\n",
    "\n",
    "        x1 = max(0, min(x1, w - 1))\n",
    "        x2 = max(0, min(x2, w - 1))\n",
    "        y1 = max(0, min(y1, h - 1))\n",
    "        y2 = max(0, min(y2, h - 1))\n",
    "\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            continue\n",
    "\n",
    "        crop = img[y1:y2, x1:x2]\n",
    "\n",
    "        if crop is None or crop.size == 0:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ocr_list = reader.readtext(crop, detail=0)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        text = \" \".join(ocr_list)\n",
    "        cleaned = extract_col_strict(text)\n",
    "\n",
    "        if cleaned:\n",
    "            found.extend(cleaned)\n",
    "\n",
    "    return sorted(list(set(found)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PIPELINE HÍBRIDO SAM → DINO\n",
    "\n",
    "def extract_col_hybrid(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    # Primero intentar con SAM\n",
    "    sam_result = sam_extract(img)\n",
    "    if sam_result:\n",
    "        return sam_result\n",
    "\n",
    "    # Si SAM falla → usar DINO\n",
    "    return dino_fallback(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando imágenes: 100%|██████████| 50/50 [06:22<00:00,  7.66s/it]\n"
     ]
    }
   ],
   "source": [
    "#PROCESAR SOLO 50 IMÁGENES + BARRA DE PROGRESO\n",
    "\n",
    "all_images = sorted([\n",
    "    f for f in os.listdir(IMAGE_DIR)\n",
    "    if f.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".jfif\"))\n",
    "])\n",
    "\n",
    "subset_images = all_images[:50]\n",
    "\n",
    "rows = []\n",
    "\n",
    "for img_name in tqdm(subset_images, desc=\"Procesando imágenes\"):\n",
    "    path = os.path.join(IMAGE_DIR, img_name)\n",
    "    detected = extract_col_hybrid(path)\n",
    "\n",
    "    col1 = next((x for x in detected if x.startswith(\"COL 1\")), \"\")\n",
    "    col2 = next((x for x in detected if x.startswith(\"COL 2\")), \"\")\n",
    "    col3 = next((x for x in detected if x.startswith(\"COL 3\")), \"\")\n",
    "\n",
    "    rows.append({\"imagen\": img_name, \"col1\": col1, \"col2\": col2, \"col3\": col3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 50,\n  \"fields\": [\n    {\n      \"column\": \"imagen\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"110.png\",\n          \"134.png\",\n          \"126.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"col1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 32,\n        \"samples\": [\n          \"COL 1. 58317 26162\",\n          \"COL 1. 41803 26254\",\n          \"COL 1. 03465 37041\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"col2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 38,\n        \"samples\": [\n          \"COL 2. 16110 91871\",\n          \"COL 2. 30068 26355\",\n          \"COL 2. 51144 64548\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"col3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 37,\n        \"samples\": [\n          \"COL 3. 00359 60964\",\n          \"COL 3. 94567 76134\",\n          \"COL 3. 91826 13568\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df_results"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-0e116385-6505-497c-860b-3a0bfda76562\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imagen</th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.jpeg</td>\n",
       "      <td>COL 1. 15355 50899</td>\n",
       "      <td>COL 2. 01146 94344</td>\n",
       "      <td>COL 3. 01816 44947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.jpeg</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.jpg</td>\n",
       "      <td></td>\n",
       "      <td>COL 2. 30669 56415</td>\n",
       "      <td>COL 3. 69800 86149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101.png</td>\n",
       "      <td>COL 1. 96476 95643</td>\n",
       "      <td>COL 2. 00069 36425</td>\n",
       "      <td>COL 3. 60880 89364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102.png</td>\n",
       "      <td></td>\n",
       "      <td>COL 2. 51144 64548</td>\n",
       "      <td>COL 3. 91826 13568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e116385-6505-497c-860b-3a0bfda76562')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-0e116385-6505-497c-860b-3a0bfda76562 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-0e116385-6505-497c-860b-3a0bfda76562');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "    imagen                col1                col2                col3\n",
       "0   1.jpeg  COL 1. 15355 50899  COL 2. 01146 94344  COL 3. 01816 44947\n",
       "1  10.jpeg                                                            \n",
       "2  100.jpg                      COL 2. 30669 56415  COL 3. 69800 86149\n",
       "3  101.png  COL 1. 96476 95643  COL 2. 00069 36425  COL 3. 60880 89364\n",
       "4  102.png                      COL 2. 51144 64548  COL 3. 91826 13568"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TABLA FINAL DE RESULTADOS\n",
    "\n",
    "df_results = pd.DataFrame(rows)\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nbformat\n",
    "import requests\n",
    "from google.colab import _message\n",
    "\n",
    "# Obtener el contenido del notebook actual\n",
    "nb_json = _message.blocking_request('get_ipynb')['ipynb']\n",
    "nb = nbformat.from_dict(nb_json)\n",
    "\n",
    "# Limpiar metadata\n",
    "if \"widgets\" in nb[\"metadata\"]:\n",
    "    del nb[\"metadata\"][\"widgets\"]\n",
    "\n",
    "for cell in nb[\"cells\"]:\n",
    "    if \"metadata\" in cell:\n",
    "        cell[\"metadata\"] = {}\n",
    "\n",
    "# Guardar archivo limpio\n",
    "output_name = \"notebook_limpio.ipynb\"\n",
    "with open(output_name, \"w\", encoding=\"utf-8\") as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "output_name"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
